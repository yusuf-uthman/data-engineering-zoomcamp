{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b89d58f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a68f4ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/06 07:49:03 WARN Utils: Your hostname, UTHMAN-PC resolves to a loopback address: 127.0.1.1; using 172.31.68.63 instead (on interface eth0)\n",
      "22/03/06 07:49:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/yusufuthman57/bin/spark-3.0.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.0.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/03/06 07:49:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f54bc08d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3e37261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'week_5_homework.ipynb',\n",
       " '03_test.ipynb',\n",
       " 'download_data.sh',\n",
       " '04_pyspark.ipynb',\n",
       " 'test.sh',\n",
       " 'taxi+_zone_lookup.csv',\n",
       " '06_spark_sql.ipynb',\n",
       " '.ipynb_checkpoints',\n",
       " '08_rdds.ipynb',\n",
       " 'head.csv',\n",
       " '05_taxi_schema.ipynb',\n",
       " '07_groupby_join.ipynb',\n",
       " 'spark-warehouse']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db040d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "492772 data/raw/fhvhv/2021/02/fhvhv_tripdata_2021_02.csv.gz\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l data/raw/fhvhv/2021/02/fhvhv_tripdata_2021_02.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc69a8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !head -n 1000 fhvhv_tripdata_2021-01.csv > head.csv\n",
    "# !wc -l head.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1110144",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = pd.read_csv(\"data/raw/fhvhv/2021/02/fhvhv_tripdata_2021_02.csv.gz\", nrows=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94d8f653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hvfhs_license_num        object\n",
       "dispatching_base_num     object\n",
       "pickup_datetime          object\n",
       "dropoff_datetime         object\n",
       "PULocationID              int64\n",
       "DOLocationID              int64\n",
       "SR_Flag                 float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c74ab3a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(hvfhs_license_num,StringType,true),StructField(dispatching_base_num,StringType,true),StructField(pickup_datetime,StringType,true),StructField(dropoff_datetime,StringType,true),StructField(PULocationID,LongType,true),StructField(DOLocationID,LongType,true),StructField(SR_Flag,DoubleType,true)))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(df_pandas).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bef9f522",
   "metadata": {},
   "outputs": [],
   "source": [
    "fhvhv_schema = types.StructType([\n",
    "    types.StructField('hvfhs_license_num', types.StringType(), True),\n",
    "    types.StructField('dispatching_base_num', types.StringType() , True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType() , True),\n",
    "    types.StructField('dropoff_datetime', types.TimestampType() , True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.DoubleType(), True)\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf968b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x='1234'\n",
    "# for i in range(len(x)):\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c44eb0d",
   "metadata": {},
   "source": [
    "### Testing the data directories to make sure they are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8eecad35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2021/1\n",
      "data/raw/fhvhv/2021/01/\n",
      "data/pq/fhvhv/2021/01/\n",
      "processing data for 2021/2\n",
      "data/raw/fhvhv/2021/02/\n",
      "data/pq/fhvhv/2021/02/\n",
      "processing data for 2021/3\n",
      "data/raw/fhvhv/2021/03/\n",
      "data/pq/fhvhv/2021/03/\n"
     ]
    }
   ],
   "source": [
    "year = 2021\n",
    "taxi_type = 'fhvhv'\n",
    "\n",
    "for month in range(1, 4):\n",
    "    print(f'processing data for {year}/{month}')\n",
    "\n",
    "    input_path = f'data/raw/{taxi_type}/{year}/{month:02d}/'\n",
    "    output_path = f'data/pq/{taxi_type}/{year}/{month:02d}/'\n",
    "    print(input_path)\n",
    "    print(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b6098a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_path = f'data/raw/fhvhv/fhvhv_tripdata_2021-01.csv.gz'\n",
    "# output_path = f'data/pq/fhvhv/2021/01/'\n",
    "\n",
    "year = 2021\n",
    "taxi_type = 'fhvhv'\n",
    "\n",
    "# processing data for Month 1 to 3 (Range does not include the last number in the loop for some reasons i dont understand yet)\n",
    "for month in range(1, 4):\n",
    "    print(f'processing data for {year}/{month}')\n",
    "\n",
    "    input_path = f'data/raw/{taxi_type}/{year}/{month:02d}/'\n",
    "    output_path = f'data/pq/{taxi_type}/{year}/{month:02d}/'\n",
    "    \n",
    "    #Reading Compressed csv files into Spark RDD\n",
    "    df_fhvhv = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(fhvhv_schema) \\\n",
    "    .csv(input_path)\n",
    "    \n",
    "    #Writing to parquet\n",
    "    df_fhvhv \\\n",
    "    .repartition(24) \\\n",
    "    .write.parquet(output_path,mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af607164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#fhvhv\n",
    "df_fhvhv = spark.read.parquet('data/pq/fhvhv/2021/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2146b0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#taxi_zone_lookup\n",
    "df_zones = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('data/raw/zones/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31338c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zones.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a353f337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fhvhv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc18413b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0003|              B02872|2021-03-25 12:13:34|2021-03-25 12:22:28|         181|         189|   null|\n",
      "|           HV0003|              B02866|2021-03-31 08:36:43|2021-03-31 09:01:44|         164|         138|   null|\n",
      "|           HV0005|              B02510|2021-03-20 14:07:13|2021-03-20 14:31:28|          37|         188|   null|\n",
      "|           HV0005|              B02510|2021-03-06 18:05:13|2021-03-06 18:16:21|          79|         164|   null|\n",
      "|           HV0003|              B02888|2021-03-28 01:27:35|2021-03-28 01:38:26|         107|         226|   null|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fhvhv.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3789ced",
   "metadata": {},
   "source": [
    "#### Adding new columns to the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "38d9d148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+--------------------+-----------+-------------------+------------+-------------------+------------+------------+-------+\n",
      "|service_type|hvfhs_license_num|dispatching_base_num|pickup_date|    pickup_datetime|dropoff_date|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+------------+-----------------+--------------------+-----------+-------------------+------------+-------------------+------------+------------+-------+\n",
      "|       fhvhv|           HV0003|              B02872| 2021-03-25|2021-03-25 12:13:34|  2021-03-25|2021-03-25 12:22:28|         181|         189|   null|\n",
      "|       fhvhv|           HV0003|              B02866| 2021-03-31|2021-03-31 08:36:43|  2021-03-31|2021-03-31 09:01:44|         164|         138|   null|\n",
      "|       fhvhv|           HV0005|              B02510| 2021-03-20|2021-03-20 14:07:13|  2021-03-20|2021-03-20 14:31:28|          37|         188|   null|\n",
      "|       fhvhv|           HV0005|              B02510| 2021-03-06|2021-03-06 18:05:13|  2021-03-06|2021-03-06 18:16:21|          79|         164|   null|\n",
      "|       fhvhv|           HV0003|              B02888| 2021-03-28|2021-03-28 01:27:35|  2021-03-28|2021-03-28 01:38:26|         107|         226|   null|\n",
      "+------------+-----------------+--------------------+-----------+-------------------+------------+-------------------+------------+------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fhvhv \\\n",
    "    .withColumn('pickup_date', F.to_date(df_fhvhv.pickup_datetime)) \\\n",
    "    .withColumn('dropoff_date', F.to_date(df_fhvhv.dropoff_datetime)) \\\n",
    "    .withColumn('service_type', F.lit('fhvhv')) \\\n",
    "    .select('service_type', 'hvfhs_license_num', 'dispatching_base_num', 'pickup_date', 'pickup_datetime', 'dropoff_date', \\\n",
    "           'dropoff_datetime', 'PULocationID', 'DOLocationID', 'SR_Flag') \\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d95947",
   "metadata": {},
   "source": [
    "#### Creatting a new RDD after Adding new columns(pickup_date, dropoff_date, service_type etc) to the initial RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f78eeeb3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_fhvhv_sel = df_fhvhv \\\n",
    "    .withColumn('pickup_date', F.to_date(df_fhvhv.pickup_datetime)) \\\n",
    "    .withColumn('dropoff_date', F.to_date(df_fhvhv.pickup_datetime)) \\\n",
    "    .withColumn('service_type', F.lit('fhvhv')) \\\n",
    "    .select('service_type', 'hvfhs_license_num', 'dispatching_base_num', 'pickup_date', 'pickup_datetime', 'dropoff_date', \\\n",
    "           'dropoff_datetime', 'PULocationID', 'DOLocationID', 'SR_Flag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ecafa44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+--------------------+-----------+-------------------+------------+-------------------+------------+------------+-------+\n",
      "|service_type|hvfhs_license_num|dispatching_base_num|pickup_date|    pickup_datetime|dropoff_date|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+------------+-----------------+--------------------+-----------+-------------------+------------+-------------------+------------+------------+-------+\n",
      "|       fhvhv|           HV0003|              B02872| 2021-03-25|2021-03-25 12:13:34|  2021-03-25|2021-03-25 12:22:28|         181|         189|   null|\n",
      "|       fhvhv|           HV0003|              B02866| 2021-03-31|2021-03-31 08:36:43|  2021-03-31|2021-03-31 09:01:44|         164|         138|   null|\n",
      "|       fhvhv|           HV0005|              B02510| 2021-03-20|2021-03-20 14:07:13|  2021-03-20|2021-03-20 14:31:28|          37|         188|   null|\n",
      "|       fhvhv|           HV0005|              B02510| 2021-03-06|2021-03-06 18:05:13|  2021-03-06|2021-03-06 18:16:21|          79|         164|   null|\n",
      "|       fhvhv|           HV0003|              B02888| 2021-03-28|2021-03-28 01:27:35|  2021-03-28|2021-03-28 01:38:26|         107|         226|   null|\n",
      "+------------+-----------------+--------------------+-----------+-------------------+------------+-------------------+------------+------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fhvhv_sel.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "747bf6b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|dispatching_base_num|  count|\n",
      "+--------------------+-------+\n",
      "|              B02876| 711785|\n",
      "|              B03136|   5242|\n",
      "|              B02877| 647412|\n",
      "|              B02869|1415850|\n",
      "|              B02883| 827348|\n",
      "+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Count count of total records from 01_jan to 03_mar is 37749803\n"
     ]
    }
   ],
   "source": [
    "df_fhvhv_sel.groupBy('dispatching_base_num').count().show(5)\n",
    "print('')\n",
    "print(f'Count count of total records from 01_jan to 03_mar is {df_fhvhv_sel.count()}')\n",
    "\n",
    "#     .orderBy('dispatching_base_num', 'count') \\\n",
    "#     .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b601c48b",
   "metadata": {},
   "source": [
    "#### Registering Tables so they can be visible as SQL Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "37ef4ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fhvhv\n",
    "df_fhvhv_sel.registerTempTable('fhvhv_trips_data')\n",
    "#zones\n",
    "df_zones.registerTempTable('zones')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188e8ef5",
   "metadata": {},
   "source": [
    "##### test SQL Statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "920c5efb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:========================================>             (150 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------+\n",
      "|service_type|dispatching_base_num|   count|\n",
      "+------------+--------------------+--------+\n",
      "|       fhvhv|              B02510|10266577|\n",
      "|       fhvhv|              B02764| 3174154|\n",
      "|       fhvhv|              B02872| 2913418|\n",
      "|       fhvhv|              B02875| 2244522|\n",
      "|       fhvhv|              B02765| 1828172|\n",
      "|       fhvhv|              B02869| 1415850|\n",
      "|       fhvhv|              B02887| 1054524|\n",
      "|       fhvhv|              B02866| 1022875|\n",
      "|       fhvhv|              B02871| 1010555|\n",
      "|       fhvhv|              B02864| 1008413|\n",
      "+------------+--------------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    service_type,\n",
    "    dispatching_base_num,\n",
    "    count(1) as count\n",
    "FROM \n",
    "    fhvhv_trips_data\n",
    "GROUP BY \n",
    "    service_type, dispatching_base_num\n",
    "ORDER BY\n",
    "    count desc\n",
    "LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf8c319",
   "metadata": {},
   "source": [
    "### Question 3 \n",
    "#### How many taxi trips were there on February 15?, Consider only trips that started on February 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "83d1b954",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------+\n",
      "|service_type|pickup_date| count|\n",
      "+------------+-----------+------+\n",
      "|       fhvhv| 2021-02-15|367170|\n",
      "+------------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    service_type,\n",
    "    pickup_date,\n",
    "    count(1) as count\n",
    "FROM \n",
    "    fhvhv_trips_data\n",
    "WHERE \n",
    "    pickup_date = '2021-02-15'\n",
    "GROUP BY \n",
    "    service_type, pickup_date\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93827cf4",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "##### Now calculate the duration for each trip.\n",
    "##### Trip starting on which day was the longest? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25bb39c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:=============================================>            (7 + 2) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+--------------------+-----------+------------+---------------------+\n",
      "|service_type|hvfhs_license_num|dispatching_base_num|pickup_date|dropoff_date|trip_duration_min_sec|\n",
      "+------------+-----------------+--------------------+-----------+------------+---------------------+\n",
      "|       fhvhv|           HV0005|              B02510| 2021-02-11|  2021-02-11|               1259.0|\n",
      "+------------+-----------------+--------------------+-----------+------------+---------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "     service_type\n",
    "    ,hvfhs_license_num\n",
    "    ,dispatching_base_num\n",
    "    ,pickup_date\n",
    "   -- ,pickup_datetime\n",
    "    ,dropoff_date\n",
    "    --,dropoff_datetime\n",
    "    --,PULocationID\n",
    "    --,DOLocationID\n",
    "    --,SR_Flag\n",
    "    ,round(((bigint(dropoff_datetime))-(bigint(pickup_datetime)))/(60),2) trip_duration_min_sec\n",
    "FROM \n",
    "    fhvhv_trips_data\n",
    "WHERE \n",
    "    pickup_date between '2021-02-01' AND '2021-02-28'\n",
    "ORDER BY \n",
    "    trip_duration_min_sec DESC\n",
    "LIMIT \n",
    "    1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c152c0ec",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "##### Now find the most frequently occurring `dispatching_base_num` in this dataset.\n",
    "\n",
    "##### How many stages this spark job has?\n",
    "\n",
    "- Note: the answer may depend on how you write the query,\n",
    "- so there are multiple correct answers. \n",
    "- Select the one you have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4d8ef44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 38:=============================================>            (7 + 2) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-------+\n",
      "|service_type|dispatching_base_num|  count|\n",
      "+------------+--------------------+-------+\n",
      "|       fhvhv|              B02510|3233664|\n",
      "+------------+--------------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "     service_type\n",
    "    ,dispatching_base_num\n",
    "    ,count(1) count\n",
    "FROM \n",
    "    fhvhv_trips_data\n",
    "WHERE \n",
    "    pickup_date between '2021-02-01' AND '2021-02-28'\n",
    "GROUP BY \n",
    "    service_type, dispatching_base_num\n",
    "ORDER BY \n",
    "    count DESC\n",
    "LIMIT \n",
    "    1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933a8597",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "##### Find the most common pickup-dropoff pair. \n",
    "\n",
    "##### For example:\n",
    "- \"Jamaica Bay / Clinton East\"\n",
    "- Enter two zone names separated by a slash\n",
    "- If any of the zone names are unknown (missing), use \"Unknown\". For example, \"Unknown / Clinton East\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "41a769c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 125:============================================>            (7 + 2) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-----+\n",
      "|service_type| pickup_dropoff_pair|count|\n",
      "+------------+--------------------+-----+\n",
      "|       fhvhv|East New York / E...|45041|\n",
      "|       fhvhv|Borough Park / Bo...|37329|\n",
      "|       fhvhv| Canarsie / Canarsie|28026|\n",
      "|       fhvhv|Crown Heights Nor...|25976|\n",
      "|       fhvhv|Bay Ridge / Bay R...|17934|\n",
      "|       fhvhv|Jackson Heights /...|14688|\n",
      "|       fhvhv|   Astoria / Astoria|14688|\n",
      "|       fhvhv|Central Harlem No...|14481|\n",
      "|       fhvhv|Bushwick South / ...|14424|\n",
      "|       fhvhv|Flatbush/Ditmas P...|13976|\n",
      "|       fhvhv|South Ozone Park ...|13716|\n",
      "|       fhvhv|Brownsville / Bro...|12829|\n",
      "|       fhvhv|    JFK Airport / NA|12542|\n",
      "|       fhvhv|Prospect-Lefferts...|11814|\n",
      "|       fhvhv|Forest Hills / Fo...|11548|\n",
      "|       fhvhv|Bushwick North / ...|11491|\n",
      "|       fhvhv|Bushwick South / ...|11487|\n",
      "|       fhvhv|Crown Heights Nor...|11462|\n",
      "|       fhvhv|Crown Heights Nor...|11342|\n",
      "|       fhvhv|Prospect-Lefferts...|11308|\n",
      "+------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# wk_5_q6_ans = \n",
    "spark.sql(\"\"\"\n",
    "--select * from (\n",
    "SELECT \n",
    "     service_type\n",
    "    ,NVL(b.Zone,'Unknown') ||' / ' || NVL(c.Zone,'Unknown') as pickup_dropoff_pair\n",
    "    ,count(1)count\n",
    "FROM \n",
    "    fhvhv_trips_data a \n",
    "    INNER JOIN   zones b\n",
    "    ON a.PULocationID = b.LocationID\n",
    "    INNER JOIN   zones c\n",
    "    ON a.DOLocationID = c.LocationID\n",
    "WHERE \n",
    "    pickup_date between '2021-02-01' AND '2021-02-28'\n",
    "   -- and (b.LocationID is null or c.LocationID is null)\n",
    "GROUP BY \n",
    "    service_type\n",
    "    ,NVL(b.Zone,'Unknown') ||' / ' || NVL(c.Zone,'Unknown')\n",
    "ORDER BY \n",
    "    count DESC\n",
    "LIMIT\n",
    "    100\n",
    "--)\n",
    "-- LOWER(pickup_dropoff_pair) like '%unknown%'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f512b358",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "wk_5_q6_ans.write.csv('data/raw/fhvhv/homework/wk_5_q6_ans', header=True, mode= 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1bac30",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "   *\n",
    "FROM \n",
    "    zones\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4942c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
